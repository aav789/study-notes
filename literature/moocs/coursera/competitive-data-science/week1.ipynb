{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of main ML algorithms\n",
    "\n",
    "### Linear models\n",
    "\n",
    "* Examples:\n",
    "  * Logistic regression\n",
    "  * Support Vector machines\n",
    "\n",
    "* Useful for sparse high-dimensionality data.\n",
    "\n",
    "* For some data, relationships are not linear and therefore not useful.\n",
    "\n",
    "### Tree-based\n",
    "\n",
    "* Examples:\n",
    "  * Decision Tree\n",
    "  * Random Forest\n",
    "  * GBDT\n",
    "  \n",
    "* Uses \"divide and conquer to recursively split spaces into subspaces\".\n",
    "* Finds \"splits\" or decisions that can be used to make inferences on that data.\n",
    "\n",
    "* For tabular/structured data: winners almost always use this approach.\n",
    "\n",
    "* Can be harder to find linear dependancies.\n",
    "\n",
    "* Scikit-learn has RandomForests.\n",
    "* XGBoost - very common approach.\n",
    "* LightGBM - also very common.\n",
    "\n",
    "### kNN-based methods\n",
    "\n",
    "* Closer objects likely to have same labels.\n",
    "* Finds \"k\" nearest neighbours.\n",
    "\n",
    "### Neural networks\n",
    "\n",
    "* Important for image and natural language processing.\n",
    "* Sometimes used with structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preprocessing and generation with respect to models\n",
    "\n",
    "### Overview\n",
    "\n",
    "* Careful preprocessing can give you edge.\n",
    "* Feature types:\n",
    "  * Id - unique to each row.\n",
    "* Feature preprocessing\n",
    "  * Model dependant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric features\n",
    "\n",
    "#### Tree-based models\n",
    "\n",
    "* Generally doesn't depend on scale of data.\n",
    "\n",
    "#### Non-tree based model\n",
    "\n",
    "* Linear models, neural networks generally want to be scaled.\n",
    "* MinMaxScaler: a good approach to bring values between 0 and 1.\n",
    "* StandardScaler: mean=0, std=1.\n",
    "* Removing outliers a good idea in linear datasets.\n",
    "  * Winsorization: process of clipping 1st and 99th percentiles.\n",
    "  \n",
    "* Rank transformation:\n",
    "  * Set spaces between sorted values to be equal.\n",
    "  * Convert values to their percentile values:\n",
    "    * rank([1000, 1, 10]) [2, 0, 1]\n",
    "    * Alternative to calculating ranks.\n",
    "    \n",
    "* Log transformation:\n",
    "  * Make data normally distributed.\n",
    "  * `np.log(1 + x)`\n",
    "  * Raising to some power < 1: `np.sqrt(x + 2.3)`\n",
    "  \n",
    "#### Feature generation\n",
    "\n",
    "* Dig into the data and generate insights for features to generate.\n",
    "* Examples:\n",
    "  * Housing prices: squared meter and price can generate price for m^2.\n",
    "  * Distance: If you had vertical and horizontal distance to something, could generate combined features.\n",
    "     * Trees could figure this stuff out, but good feature engineering can result in less trees.\n",
    "       * Have trouble with division and multiplictive features.\n",
    "  * Price: could include fractional part of the price to figure out its impact on person's perception. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical and ordinal features\n",
    "\n",
    "* Titanic dataset example of categorical:\n",
    "  * Sex, Cabin, Embarked\n",
    "  * Doesn't have an underlying ordering\n",
    "* Examples of ordinal:\n",
    "  * Pclass - has a natural ordering.\n",
    "  \n",
    "* Preparing categorical variables:\n",
    "  * Label encoding:\n",
    "    * Convert to numbers.\n",
    "    * Works okay for tree based models but not for linear models\n",
    "    * `sklearn.preprocessing.LabelEncoding` - orders them alphabetical\n",
    "    * `Pandas.factorize` - orders them based on when it saw them in the data.\n",
    "  * Frequency encoding: order by how frequently it's seen in the dataset:\n",
    "    `[S, C, Q] -> [0.5, 0.3, 0.2]`\n",
    "    * Useful even for tree model, if frequency of category is correlated with target value.\n",
    "  * One-hot encoding\n",
    "    * Example:\n",
    "        * Season: \n",
    "          [`winter`,\n",
    "           `summer`,\n",
    "           `summer`\n",
    "          ]\n",
    "        * Encoding:\n",
    "          [`season_winter`: 1, `season_summer`: 2]\n",
    "    * Can utilise sparse matrixes if lots of 0 values.\n",
    "      * Useful for categorical or text data.\n",
    "  * Feature generation\n",
    "    * Interaction of categorical features can be useful for linear models and kNN.\n",
    "      * Example: `Pclass` and `sex` combined to `Pclass_sex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime and coordinates\n",
    "\n",
    "* Date time feature generation:\n",
    "  * Periodicity:\n",
    "     * Day number in week, month, season, year, second, minute, hour.\n",
    "   * Time since:\n",
    "     * Row independent: time since 1970\n",
    "     * Row-dependant: time since last public holiday.\n",
    "   * Difference:\n",
    "     * Difference between some other date (time user subscribed vs purchase).\n",
    "     \n",
    "* Co-ordinates\n",
    "  * If you had the data, could add distance to nearby landmarks: shops, hospitals, schools etc.\n",
    "  * Could extract points on the map from train / test data.\n",
    "    * Distance to most expensive place.\n",
    "  * Aggregated stats: slightly rotate coords.\n",
    "  \n",
    "### Handling missing values\n",
    "\n",
    "* Plotting histogram useful for finding numbers used as replacement value.\n",
    "  * If the distribution is normalised, with a peak somewhere, you might assume they are missing.\n",
    "* Missing value imputation:\n",
    "  1. Some number like -999 or -1\n",
    "    * Can hurt performance of linear models and neural networks.\n",
    "  2. Mean or median\n",
    "    * Good for linear models\n",
    "    * Can be hard for tree models to find which ones are missing.\n",
    "      * Solution: add another column that describes if a value is missing or not.\n",
    "  3. Reconstruct value\n",
    "    * Approximate with nearby observations.\n",
    "    * Could train another model to find features.\n",
    "  * Want to think about future feature generation when filling missing values.\n",
    "* XGBoost can handle missing values out the box.\n",
    "\n",
    "* Handling features not present in train data:\n",
    "  * Unsupervised methods like frequency encoding can be used to handle missing test data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction from text and images\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "* Create new column for each word from the data.\n",
    "* Add counts to each column for the word.\n",
    "* `CountVectorizer` takes that approach.\n",
    "* TF/IDF:\n",
    "  * Calculate term frequency:\n",
    "  \n",
    "     ```\n",
    "     tf = 1 / x.sum(axis=1)\n",
    "     x = x * tf\n",
    "     ```\n",
    "     \n",
    "  * Calculate inverse doc frequency:\n",
    "  \n",
    "    ```\n",
    "    idf = np.log(x.shape[0] / (x > 0).sum(0)\n",
    "    x = x * idf\n",
    "    ```\n",
    "    \n",
    "  * ``sklearn.feature_Extraction.text.TfidfVectorizer``\n",
    "  \n",
    "* N-grams:\n",
    "  * Have a column for each combination of n characters.\n",
    "  * `hello world`: 1, `hello`: 1, `world`: 1\n",
    "  \n",
    "* Preprocessing\n",
    "  * Lowercase: `Hello -> hello`\n",
    "  * Stemming: `democracy, democratic, democratization -> democr`\n",
    "    * Less careful preprocessing and doesn't require knowledge of language.\n",
    "  * Lemmatisation: `democracy, democratic, democratization -> democracy`\n",
    "  * Stop words\n",
    "    * Remove words which are insignificant or too common to be useful.\n",
    "    \n",
    "### Word2Vec, CNN\n",
    "\n",
    "* Creating embeddings of words:\n",
    "  * Word2Vec\n",
    "  * Glove\n",
    "  * FastText\n",
    "* Embeddings of sentences:\n",
    "  * Doc2Vec\n",
    "\n",
    "* BOW and embeddings can give quite different results and can be combined together.\n",
    "\n",
    "* Convolution models can reuse earlier layers on different problems.\n",
    "  * Finetuning can be used to retrain layers to solve similar but different models.\n",
    "  \n",
    "* Images can be augmented to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
