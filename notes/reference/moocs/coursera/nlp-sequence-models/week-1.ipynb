{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why sequence models\n",
    "\n",
    "* \"RNNs have transformed speech recognition, NLP and other areas.\"\n",
    "* Examples:\n",
    "  * Speech recognition: given some audio input, output sequence of words.\n",
    "  * Music generation: given an empty set, or a single integer to represent genre or some first notes, generate output music.\n",
    "  * Sentiment classification: Given some text, output the sentiment.\n",
    "  * DNA sequence analysis\n",
    "  * Machine translation.\n",
    "  * Video activity recognition.\n",
    "  * Name entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "* Motivation: Find people's names in sentence (named entity recognition).\n",
    "    * x: \"Harry Potter and Hermione Granger invented a new spell.\n",
    "    * y: ```[1 1 0 1 1 0 0 0 0]```\n",
    "    * Input (x):\n",
    "      * Sequence of 9 words, with each word denoted as $x^{<t>}$ up to $x^{<Tx>}$.\n",
    "      * $Tx$ = number of words in a single training example (9 in this example).\n",
    "      * $ x^{<1>} x^{<2>} .. x^{<9>} $\n",
    "      * $X^{(i)<t>}$ is a single char within a single training example (sentence).\n",
    "      * Denote $T_x = 9$ as num input words.\n",
    "    \n",
    "      * Denote as $y^{<t>}$ with each output denoted as $y^{<t>}$ up to $y^{<Ty>}$.\n",
    "      * $T_y$ = number output digits (9 in this example).\n",
    "      * $y^{(i)<t>}$ is output digit $t$.\n",
    "      \n",
    "* Representing words: build a vocabulary or dictionary of words to digits.\n",
    "  * ```[a, aaron, and, ..., harry, potter, ..., zulu]```\n",
    "  * Dictionary sizes of 30k to 50k are common, some could even go up to 1M words.\n",
    "  * Each word is represented as a 10k one hot encoded word vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Model\n",
    "\n",
    "* Re named entity recognition problem, why not a standard neural network?\n",
    "\n",
    "    * Inputs, outputs can be different length in different examples - standard NN needs consistent input and output.\n",
    "    * Doesn't share features learned across different *positions* of text.\n",
    "      * Eg doesn't learn that Harry in position 1 may be a person's name.\n",
    "      \n",
    "* RNN:\n",
    "\n",
    "  * Steps:\n",
    "\n",
    "      1. Take first word, $x^{<1>}$, and feed into neural network layer and predict output, $y^{<1>}$.\n",
    "      2. When it reads the second word, $x^{<2>}$, it predicts output using the activation from the last step, $a^{<1>}$ and the second word, to output $y^{<2>}$.\n",
    "  \n",
    "  * Parameters:\n",
    "    * Parameters are $Waa$, which governs the activation parameters, and $Wya$, which governs the output predictions and $Wax$ for the inputs:\n",
    "    \n",
    "  * Predictions:\n",
    "      * \n",
    "    \n",
    "    $a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$\n",
    "    $\\hat{y} = g(W_{ya}a^{<t>} + b_{y})$\n",
    "  \n",
    "  ![rnn-basic-architecture.png](attachment:rnn-basic-architecture.png)\n",
    "  \n",
    "  * This RNN describes a unidirectional architecture because it only goes one way through the network.\n",
    "    \n",
    "* Forward propagation:\n",
    "\n",
    "    ![rnn-forward-prop.png](attachment:rnn-forward-prop.png)\n",
    "\n",
    "* Notation:\n",
    "\n",
    "  * Activation param is calculated by multiplying the activation weights and previous a, then adding the input weights times the input then adding the activation bias and applying an activation function:\n",
    "\n",
    "      $a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$\n",
    "      \n",
    "      * Activation function is often `tanh`.    \n",
    "      \n",
    "  * Prediction is calculated by multiplying the output weights with the activations and adding a bias term:\n",
    "  \n",
    "      $\\hat{y} = g(W_{ya}a^{<t>} + b_{y})$\n",
    "     \n",
    "      * Activation function is often `softmax` (if multiclass) or `sigmoid` (if binary).\n",
    "  \n",
    "* Simplified notation:\n",
    "\n",
    "  * Input:\n",
    "  \n",
    "      $a^{<t>} = g(W_a[a^{<t-1>}, x^{(t)}] + ba)$\n",
    "      \n",
    "      * Notation means to take 2 vectors and stack.\n",
    "        \n",
    "  * Output:\n",
    "  \n",
    "      $\\hat{y}^{<t>} = g(W_{y}a^{<t>} + by)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation through time\n",
    "\n",
    "* Loss function for a single input: $L^{<t>}(\\hat{y}^{<t>}, y^{<t>}) = -y^{<t>}\\log \\hat{y}^{<t>} - (1 - y^{<t>}) \\log (1 - \\hat{y}^{<t>})$\n",
    "* Loss for whole training example: $L(\\hat{y}, y) = \\sum\\limits^{Ty}_{t=1} L^{<t>}(\\hat{y}^{<t>}, y^{<t>})$\n",
    "\n",
    "* Back propagation performs calculations in the opposite direction of forward propagation.\n",
    "\n",
    "![backprop-through-time.png](attachment:backprop-through-time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different types of RNN\n",
    "\n",
    "* Tx does not always equal Ty in RNNs.\n",
    "  * Speech recognition.\n",
    "  * Music generation.\n",
    "\n",
    "* One-to-one\n",
    "  * Standard neural network.\n",
    "\n",
    "* One-to-many\n",
    "  * Music generation.\n",
    "  * Input a single genre (or some other input including null) and output notes.\n",
    "\n",
    "* Many-to-one\n",
    "  * Sentiment classifier\n",
    "  * Inputs many words and outputs a single number.\n",
    " \n",
    "* Many-to-many\n",
    "  * Named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model and sequence generation\n",
    "\n",
    "* How could speech recognition differentiate between: \"the apple and pair salad\" and \"the apple and pear salad\"?\n",
    "\n",
    "  * Answer: language model.\n",
    "  * P(the apple and pair salad) = 3.2 * 10^-13\n",
    "  * P(the apple and pear salad) = 5.7 * 10^-10\n",
    "  * P(sentence) = ?\n",
    "  \n",
    "* To build with RNN:\n",
    "\n",
    "  1. Get a large corpus of English text and tokenize (Extra token for end of a sentence?)\n",
    "  2. Attempt to predict the first word (using just probability), then predict the 2nd word using the first word and so on.\n",
    "  \n",
    "* Model can predict the probability of the next current word given the words before it:\n",
    "\n",
    "  $P(y^{<3>}|y^{<1>},y^{<2>})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling novel sequences\n",
    "\n",
    "* After training sequence model, can get an idea of what it's learned by generating random sentences.\n",
    "  * At first time step, take a word according to distribution of softmax output (basically, randomly pick a likely word).\n",
    "  * Input that word to the next step to get another prediction.\n",
    "  * Keep repeating until you can an EOS token.\n",
    "  * May want to reject any unknown word tokens: keep sampling until you get an actual word.\n",
    "\n",
    "* Character-level language model:\n",
    "  * Use each character rather than each word from training dataset to build model.\n",
    "  * Can handle all words in input: no words are assigned 0 probability.\n",
    "  * Harder to train and more computationally expense.\n",
    "  * Used in more specialised applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Vanishing gradients with RNN\n",
    "\n",
    "* Gradients need to learn very long-term dependancies. The \"was\" or \"were\" in the following sentences are depends on the plurality of \"cat\", so you need to remember you said \"cat\" for a while.\n",
    "  * The cat, which already ate .. ..., was full.\n",
    "  * The cats, ..., were full.\n",
    "    \n",
    "* With standard RNNs, points in sequences are heavily influenced by points nearby - hard to learn long term dependancies.\n",
    "* RNNs also have the problem of exploding gradients, but vanishing tends to be a bigger problem.\n",
    "  * Tend to see NaNs with exploding gradients due to numeric overflow.\n",
    "    * Can use \"gradient clipping\" to solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)\n",
    "\n",
    "* Revisit formula for computing activations:\n",
    "\n",
    "    ![rnn-revisited.png](attachment:rnn-revisited.png)\n",
    "    \n",
    "* GRU introduces a new variable $c$ which standards for cell (memory cell).\n",
    "* At time t, the memory cell will have some value $c^{<t>}$ which is the same as the activation: $c^{<t>} = a^{<t>}$.\n",
    "* At every timestep, we consider overwriting memory cell with value $\\tilde{c}^{<t>}$ using the following formula:\n",
    "\n",
    "  * $\\tilde{c}^{<t>} = tanh(W_c[c^{<t-1>}, x^{<t>}] + b)$\n",
    "  * $\\Gamma u = \\sigma(W_u[c^{<t-1>}, x^{<t>} + bu)$\n",
    "    * Sigmoid function so values are between 0 and 1.\n",
    "\n",
    "      * The memory gate is supposed to decide when to update the $c^{<t>}$ value (ie when we pass \"cat\" in \"the cat, which already ate .. ..., was full.\").\n",
    "      * For all values before the \"was\" in the above sentence, gate would equal 0.\n",
    "   * c is updated as follows:\n",
    "     \n",
    "     $c^{<t>}=\\Gamma u * \\tilde{c}^{<t>} + (1-\\Gamma u) * c^{<t-1>}$\n",
    "       * When $\\Gamma u = 0$ (or close to it) then you don't update the value, when $\\Gamma u = 1$ then it's updated.\n",
    "       \n",
    "* A full GRU includes another gate $\\Gamma r$ which calculates the \"relevance\" of candidate $c^{<t-1>}$.\n",
    "\n",
    "![full-gru.png](attachment:full-gru.png)\n",
    "\n",
    "  * Note: I don't fully understand this material at the time of writing these notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)\n",
    "\n",
    "* \"Even more powerful than the GRU\".\n",
    "* Also has a memory cell and a update gate, but differs in that it includes a \"forget gate\":\n",
    "  \n",
    "    $\\tilde{c}^{<t>} = \\tanh(W_c[a^{<t-1>}, x^{<t>}]+bc)\\\\\n",
    "    \\Gamma u = \\sigma(W_u[a^{<t-1>}, x^{<t>}] + bu)\\\\\n",
    "    \\Gamma f = \\sigma(W_f[a^{<t-1>}, x^{<t>}] + bf)\\\\\n",
    "    \\Gamma o = \\sigma(W_o[a^{<t-1>}, x^{<t>}] + bo)\\\\\n",
    "    c^{<t>} = \\Gamma u * \\tilde{c}^{<t>} + \\Gamma f * c^{<t-1>}\\\\\n",
    "    a^{<t>} = \\Gamma o * \\tanh c^{<t>}$\n",
    "\n",
    "\n",
    "* LSTM (and GRU) can pass on candidate value very easily from neuron to neuron.\n",
    "\n",
    "* \"peephole connection\" is a common variation of LSTMs where the candidate value can be affected by earlier inputs (not 100% sure about this).\n",
    "\n",
    "* GRUs is a more recent invention, as a simplification of LSTM, and they both tend to work for different problems.\n",
    "  * GRU tends to be faster and easier to train.\n",
    "  * LSTM tends to be more accuracy - and is often the default choice.\n",
    "  \n",
    "* Question: what is the dimension of $\\Gamma u$ at each time step when you have 10k voc?\n",
    "\n",
    "* Note: I don't fully understand this material at the time of writing these notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN\n",
    "\n",
    "* Some problems require a backwards pass (not back prop) through the RNN, as well as a forward.\n",
    "* Can add a \"backward recurrent layer\", which add another set of activations that work backward through the graph.\n",
    "* Output at each node takes into account the forward and backward calculated activations: $\\hat{y}^{<t>}=g(Wy[\\overrightarrow{a}^{<t>}, \\overleftarrow{a}^{<t>}] + by)$\n",
    "* BRNN with LSTM blocks is quite common in NLP problems\n",
    "  * Not so good for real-time speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNNs\n",
    "\n",
    "* Can be useful to stack multiple RNN layers to create deeper models.\n",
    "* Notation below adds square bracket to denote the layer: $a[1]<0>$ = activation 0 on the first layer.\n",
    "* A single cell is calculated by taking activations from left layer and bottom layer.\n",
    "\n",
    "<img src=\"./deep-rnn.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
